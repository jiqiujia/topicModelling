# encoding=utf-8
from __future__ import unicode_literals, print_function, division


import re
import jpype
from term import Term

pattern_4byte_char = re.compile(u'[^\u0000-\uD7FF\uE000-\uFFFF]', re.UNICODE)


class HanlpStandardTokenizer:
    """
    Hanlpåˆ†è¯çš„å°è£…ç±»
    TODO: è‡ªå®šä¹‰å¯ç”¨hanlpåˆ†è¯å™¨åŠŸèƒ½å¦‚äººååœ°åè¯†åˆ«ç­‰
    """

    def __init__(self, path_class, data_dir='', remove_4byte_char=True):
        """
        åˆå§‹åŒ–
        :param path_class: jaråŒ…ã€èµ„æºæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹å¦‚ä¸‹
            path_class_win = "-Djava.class.path=C:\hanlp\hanlp-1.2.8.jar;C:\hanlp"
            path_class_nix = "-Djava.class.path=/Users/nathanlvzs/data/hanlp/hanlp-1.3.4.jar:/Users/nathanlvzs/data/hanlp"
        :param data_dir: æ•°æ®æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼Œå³åŒ…å«dataå­æ–‡ä»¶å¤¹çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œ
            ä»…åœ¨ä½¿ç”¨dphanlpæ—¶æŒ‡å®šè¯¥å‚æ•°ï¼Œæ­¤æ—¶path_classä¸­æŒ‡å®šjarå³å¯
        """
        if not jpype.isJVMStarted():
            print('JVM started! Closing...')
            #self.dispose()
            jpype.startJVM(jpype.getDefaultJVMPath(), '-ea', path_class, "-Xms1g", "-Xmx2g")
        print(jpype.getClassPath())
        print(jpype.isJVMStarted())
        self.tokenizer = jpype.JClass('com.hankcs.hanlp.HanLP')
        #self.CRFSegment = jpype.JClass('com.hankcs.hanlp.seg.CRF.CRFSegment')()
        self.stop_word_dictionary = jpype.JClass('com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary')
        self.remove_4byte_char = remove_4byte_char
        # if data_dir:
        #     hanlp_config = jpype.JClass('com.hankcs.hanlp.HanLP$Config')
        #     hanlp_config.LoadResouce(data_dir)
        pass

    def _clean(self, line):
        # reference:
        # https://stackoverflow.com/questions/3220031/how-to-filter-or-replace-unicode-characters-that-would-take-more-than-3-bytes
        if self.remove_4byte_char:
            return pattern_4byte_char.sub('', line)
        else:
            return line

    def cut_with_nature(self, line, rm_stop_word=False):
        """
        åˆ†è¯
        :param line: è¾“å…¥æ–‡å­—
        :return: Termåˆ—è¡¨ï¼ŒTermå®ä¾‹åŒ…å«wordå’Œnatureä¸¤ä¸ªæˆå‘˜
        :param rm_stop_word: æ˜¯å¦å»é™¤åœç•™è¯
        """
        line = self._clean(line)
        result = []
        raw_result = self.tokenizer.segment(jpype.JString(line))
        if rm_stop_word:
            self.stop_word_dictionary.apply(raw_result)
        # for token in self.tokenizer.segment(JString(line)):
        # <class 'jpype._jclass.java.util.ArrayList'>
        # result.append(Term(token.word, token.nature.toString()))
        for token in raw_result:
            # <class 'jpype._jclass.java.util.ArrayList'>
            result.append(Term(token.word, token.nature.toString()))
        return result

    # def cut_with_crf(self, line, rm_stop_word=False):
    #     line = self._clean(line)
    #     result = []
    #     raw_result = self.CRFSegment.seg(jpype.JString(line))
    #     if rm_stop_word:
    #         self.stop_word_dictionary.apply(raw_result)
    #     for token in raw_result:
    #         # <class 'jpype._jclass.java.util.ArrayList'>
    #         result.append(Term(token.word, token.nature.toString()))
    #     return result
    
    def cut(self, line, rm_stop_word=False):
        """
        åˆ†è¯
        :param line: è¾“å…¥æ–‡å­—
        :return: åˆ†è¯ç»“æœå­—ç¬¦ä¸²åˆ—è¡¨
        :param rm_stop_word: æ˜¯å¦å»é™¤åœç•™è¯
        """
        # å¯¹äºâ€œLet's Go!!!ğŸ™ğŸ°ğŸœğŸŸğŸ¢ğŸ£ğŸ•ğŸ¤......â€åˆ‡è¯ä¼šå‡ºHeapCorruptionçš„å¼‚å¸¸ï¼Œç¨‹åºç›´æ¥æŒ‚
        # Python3 jpype exit code -1073740940
        # emoji unicodeå­—ç¬¦é›†ï¼šhttp://unicode.org/Public/emoji/11.0/
        # å•ç‹¬åˆ‡ï¼Œä¸€ä¸¤æ¬¡æ²¡é—®é¢˜ï¼Œé‡å¤å‡ æ¬¡å°±æœ‰é—®é¢˜
        # åœ¨javaç«¯æµ‹è¯•å¹¶æ²¡æœ‰å‡ºé”™
        # 4byteå­—ç¬¦æ²¡æœ‰è¢«å‡†ç¡®è½¬æ¢ï¼Ÿæš‚ä¸”é€šè¿‡_cleanæ–¹æ³•å»é™¤4byteå­—ç¬¦
        # multi-byte characters between Java and Python
        # line = self._clean(line)
        # result = []
        # for token in self.tokenizer.segment(JString(line)):
        #     result.append(token.word)
        result = []
        for term in self.cut_with_nature(line, rm_stop_word):
            result.append(term.word)
        return result

    def add_custom_words(self, words):
        custom_dictionary = jpype.JClass('com.hankcs.hanlp.dictionary.CustomDictionary')
        for word in words:
            word = self._clean(word)
            custom_dictionary.add(jpype.JString(word))

    def add_stop_words(self, words):
        for word in words:
            word = self._clean(word)
            self.stop_word_dictionary.add(word)
        pass

    def dispose(self):
        """
        é”€æ¯JVMèµ„æºï¼Œè°ƒç”¨åï¼Œå°±æ— æ³•å†ä½¿ç”¨segmentæ–¹æ³•äº†
        :return:
        """
        jpype.shutdownJVM()


if __name__ == '__main__':
    hanlp_path = "-Djava.class.path=hanlp-1.6.8.jar;."
    segmenter = HanlpStandardTokenizer(hanlp_path, data_dir="E:\\libs\\dphanlp\\data")
    segmenter.add_stop_words(['äºšæ´²'])
    segmenter.add_custom_words(['å¿…å»çš„'])
    text = "ã€åƒè´§æ”»ç•¥ã€‘å¿…å»çš„äºšæ´²ç¾é£Ÿé›†å¸‚è§…é£Ÿä¹‹æ—…ğŸ™ğŸ°ğŸœğŸŸğŸ¢ğŸ£ğŸ•ğŸ¤......"
    for token in segmenter.cut(text, True):
        print(token)
    pass
